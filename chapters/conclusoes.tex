% ----------------------------------------------------------
\chapter{Conclusões}\label{conclusoes}
% ----------------------------------------------------------

O presente trabalho teve por objetivo avaliar comparativamente o desempenho das LLM's, especificamente GPT, Gemini e DeepSeek na resolução de questões objetivas do ENADE aplicadas aos 
cursos da área de Computação. A análise, fundamentada nos gabaritos oficiais, permitiu traçar um panorama detalhado sobre a assertividade, a estabilidade e as competências específicas de cada 
modelo frente a desafios acadêmicos de nível superior.

Em resposta à primeira questão de pesquisa (QP01), sobre quão assertivas podem ser as LLM's na resolução de questões em computação, os resultados demonstraram que os modelos atuais atingiram um 
patamar elevado de competência. Observou-se uma convergência de desempenho, especialmente na temperatura 0,8, em que todos os modelos apresentaram probabilidades de acerto semelhantes e consistentes.
 Isso valida o uso dessas ferramentas como auxiliares no processo de aprendizado e revisão de conceitos fundamentais da área.

Em relação a segunda questão (QP02), referente a qual modelo obteve o melhor resultado, a análise revelou nuances importantes entre "pico de desempenho" e "consistência". 
O GPT consolidou-se como o modelo mais robusto e estável. Sua arquitetura demonstrou baixa sensibilidade à variação de temperatura, mantendo alta acurácia e recall constante (0,82\%) 
mesmo em cenários de maior aleatoriedade. Por outro lado, o Gemini e o DeepSeek, embora apresentem oscilações maiores, revelaram-se capazes de superar o GPT em condições específicas de temperatura,
 sugerindo que, quando devidamente calibrados, possuem um teto de performance ligeiramente superior para recuperação de informações.

Quanto à terceira questão (QP03), sobre a superioridade em áreas específicas, o estudo identificou especializações claras:
\begin{itemize}
    \item \textbf{GPT:} Mostrou-se superior em Análise de Algoritmos e Conhecimentos Gerais, sendo a escolha ideal para cenários que exigem consistência e baixa taxa de alucinação. No entanto, 
                        seu desempenho foi limitado em Segurança da Informação, provavelmente devido a filtros de segurança mais restritivos.
    \item \textbf{Gemini:} Destacou-se em Segurança da Informação, Fundamentos Matemáticos e Compiladores, atingindo acertos absolutos nessas categorias.
    \item \textbf{DeepSeek:} Revelou-se a melhor opção para Estruturas de Dados, sendo o único capaz de atingir 100\% de acurácia nesta área em uma temperatura criativa (2.0), 
                             além de ser uma alternativa sólida ao Gemini em matemática e segurança.
\end{itemize}

Uma limitação transversal identificada nos três modelos foi o baixo desempenho nos conteúdos de Sistemas Operacionais (acurácia entre 28\% e 57\%). Conclui-se que essa dificuldade advém da 
natureza visual e diagramática de muitas questões dessa área (estados de memória, hardware), que impõem barreiras à interpretação puramente textual das LLM's atuais em algumas áreas.

Por fim, este estudo evidencia que não há uma "melhor LLM" universal para a Computação, mas sim ferramentas com perfis complementares. Para tarefas que exigem rigor lógico e estabilidade, 
o GPT é o mais indicado; para tarefas que envolvem criatividade técnica, matemática formal ou domínios específicos como segurança, o Gemini e o DeepSeek apresentam vantagens competitivas.

Como sugestão para trabalhos futuros, recomenda-se a expansão da análise para modelos multimodais (capazes de interpretar as imagens) para analise de questões que fazem uso de recursos visuais como diagramas, gráficos ou estruturas voltadas a computação.,
 e a investigação mais aprofundada sobre como o prompt engineering pode mitigar as alucinações observadas nas temperaturas mais elevadas do DeepSeek e Gemini.